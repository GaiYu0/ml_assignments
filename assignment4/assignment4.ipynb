{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from array import array\n",
    "import math\n",
    "import numpy as np # numpy is only used to check implementation\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dot(left, right):\n",
    "    return math.fsum(l * r for l, r in zip(left, right))\n",
    "def sigmoid(X):\n",
    "    return 1 / (1 + math.exp(-X))\n",
    "def label_array(iterable):\n",
    "    return array('b', list(iterable))\n",
    "def double_array(iterable):\n",
    "    return array('d', list(iterable))\n",
    "def index_column(matrix, index):\n",
    "    return array(matrix[0].typecode, [row[index] for row in matrix])\n",
    "def index_columns(matrix, lower, upper):\n",
    "    return tuple(row[lower : upper] for row in matrix)\n",
    "def multiply(left, right):\n",
    "    # element-wise\n",
    "    return tuple(double_array(l * r for l, r in zip(left_row, right_row)) for left_row, right_row in zip(left, right))\n",
    "def matrix_dot(left, right):\n",
    "    return \\\n",
    "        tuple(double_array(dot(row, index_column(right, index)) for index in range(len(right[0]))) for row in left)\n",
    "def transpose(matrix):\n",
    "    return tuple(index_column(matrix, i) for i in range(len(matrix[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    return tuple(double_array(map(float, line.split(','))) for line in open(file, 'r'))\n",
    "def load_labels(file):\n",
    "    return label_array(map(lambda i : int(i) - 1, open(file, 'r')))\n",
    "def load_dataset():\n",
    "    return load_data('data.csv'), load_labels('data-labels.csv')\n",
    "def load_weights():\n",
    "    return transpose(load_data('theta1.csv')), transpose(load_data('theta2.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data, labels = load_dataset()\n",
    "theta1, theta2 = load_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unpack(theta):\n",
    "#     return index_columns(theta, 1, len(theta[0])), index_column(theta, 0)\n",
    "    return theta[1:], theta[0]\n",
    "def neuron_forward(data, weight, bias, activate=sigmoid):\n",
    "    '''\n",
    "    Only to fulfill the requirement of question 1.\n",
    "    The function is not going to be called because computation of neural networks can be further verctorized.\n",
    "    '''\n",
    "    return activate(dot(data, weight) + bias)\n",
    "def layer_forward(data, weight, bias, activate=sigmoid):\n",
    "    product = matrix_dot(data, weight)\n",
    "    linear = tuple(double_array(map(lambda X : X + bias[i], index_column(product, i))) for i in range(len(product[0])))\n",
    "    nonlinear = tuple(double_array(map(activate, row)) for row in transpose(linear))\n",
    "    return nonlinear\n",
    "def network_forward(data, theta1, theta2, activate=sigmoid):\n",
    "    weight1, bias1 = unpack(theta1)\n",
    "    activations = layer_forward(data, weight1, bias1, activate)\n",
    "    weight2, bias2 = unpack(theta2)\n",
    "    scores = layer_forward(activations, weight2, bias2, activate=lambda X : X)\n",
    "    cache = (data, activations, weight2)\n",
    "    return scores, cache\n",
    "def predict(scores):\n",
    "    return double_array(row.index(max(row)) for row in scores)\n",
    "def n_errors(prediction, labels):\n",
    "    return sum(prediction != label for prediction, label in zip(predictions, labels))\n",
    "def error_rate(prediction, labels):\n",
    "    return n_errors(prediction, labels) / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0248"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classification error\n",
    "scores, network_cache = network_forward(data, theta1, theta2)\n",
    "predictions = predict(scores)\n",
    "error_rate(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# negative log likelihood loss (convert class scores to probability via softmax function)\n",
    "def nll_loss(scores, labels):\n",
    "    N = len(scores)\n",
    "    maximums = double_array(map(max, scores))\n",
    "    normalized_scores = \\\n",
    "        tuple(double_array(map(lambda score : score - maximum, row)) for row, maximum in zip(scores, maximums))\n",
    "    exponents = tuple(double_array(map(math.exp, row)) for row in normalized_scores)\n",
    "    totals = double_array(math.fsum(row) for row in exponents)\n",
    "    probabilities = double_array(row[label] / total for row, label, total in zip(exponents, labels, totals))\n",
    "    negative_log = double_array(map(lambda X : 0 - math.log(X), probabilities))\n",
    "    loss = math.fsum(negative_log) / N\n",
    "    cache = (scores, labels, exponents, totals)\n",
    "    return loss, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08688856037475026"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute loss\n",
    "loss, loss_cache = nll_loss(scores, labels)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def d_nll_loss(cache):\n",
    "    # gradient w.r.t. class scores\n",
    "    scores, labels, exponents, totals = cache\n",
    "    N = len(scores)\n",
    "    d_scores = tuple(double_array(map(lambda X : X / (N * total), row)) for row, total in zip(exponents, totals))\n",
    "    for row, label in zip(d_scores, labels): row[label] -= 1 / N\n",
    "    maximum_indices = map(lambda vector : vector.index(max(vector)), scores)\n",
    "    for row, index in zip(d_scores, maximum_indices): row[index] -= math.fsum(row)\n",
    "    return d_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load internal results of vecterized implementation and use those results to check non-vectorized implementation\n",
    "cache = pickle.load(open('internal', 'rb'))\n",
    "_scores, _predictions, _d_scores, _d_theta1, _d_theta2 = cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relative_error(left, right):\n",
    "    return np.max(np.abs(left - right) / (np.abs(left) + np.abs(right)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute gradient w.r.t. class scores\n",
    "d_scores = d_nll_loss(loss_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62125377330312859"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare results yielded by vecterized and non-vecterized implementations\n",
    "relative_error(np.array(d_scores), _d_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def network_backward(d_scores, cache):\n",
    "    # gradient w.r.t. parameters\n",
    "    data, activations, weight2 = cache\n",
    "    d_bias2 = double_array(sum(index_column(d_scores, i)) for i in range(len(d_scores[0])))\n",
    "    d_weight2 = matrix_dot(transpose(activations), d_scores)\n",
    "    d_activations = matrix_dot(d_scores, transpose(weight2))\n",
    "    d_pre_activations = multiply(\n",
    "        d_activations,\n",
    "        multiply(activations, tuple(double_array(map(lambda X : 1 - X, row)) for row in activations)),\n",
    "    )\n",
    "    d_bias1 = double_array(sum(index_column(d_pre_activations, i)) for i in range(len(d_pre_activations[0])))\n",
    "    d_weight1 = matrix_dot(transpose(data), d_pre_activations)\n",
    "    return d_weight1, d_bias1, d_weight2, d_bias2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute gradient w.r.t. parameters\n",
    "d_weight1, d_bias1, d_weight2, d_bias2 = network_backward(d_scores, network_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pack(weights, biases):\n",
    "    # assemble weight and bias into theta\n",
    "    return transpose(tuple(double_array((bias,)) + row for bias, row in zip(biases, transpose(weights))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/python_official/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:2: RuntimeWarning: invalid value encountered in true_divide\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check gradient w.r.t. theta 1\n",
    "d_theta1 = pack(d_weight1, d_bias1)\n",
    "relative_error(np.array(d_theta1), _d_theta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check gradient w.r.t. theta 2\n",
    "d_theta2 = pack(d_weight2, d_bias2)\n",
    "relative_error(np.array(d_theta2), _d_theta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that gradient computed via Python arithmetic does not match gradient computed via numpy precisely. However, the statistics of Python gradients matches those of numpy gradients. (Please refer to the other notebook for statistics of numpy gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean = 0.000115\n",
      "median = 0.000033\n",
      "standard deviation = 0.000188\n"
     ]
    }
   ],
   "source": [
    "# statistics w.r.t. theta 1\n",
    "abs_d_theta1 = np.abs(d_theta1)\n",
    "mean = np.mean(abs_d_theta1)\n",
    "median = np.median(abs_d_theta1)\n",
    "std = np.std(abs_d_theta1)\n",
    "print('mean = %f' % mean)\n",
    "print('median = %f' % median)\n",
    "print('standard deviation = %f' % std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean = 0.000514\n",
      "median = 0.000379\n",
      "standard deviation = 0.000443\n"
     ]
    }
   ],
   "source": [
    "# statistics w.r.t. theta 2\n",
    "abs_d_theta2 = np.abs(d_theta2)\n",
    "mean = np.mean(abs_d_theta2)\n",
    "median = np.median(abs_d_theta2)\n",
    "std = np.std(abs_d_theta2)\n",
    "print('mean = %f' % mean)\n",
    "print('median = %f' % median)\n",
    "print('standard deviation = %f' % std)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
